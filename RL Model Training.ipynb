{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import important libraries\n--------","metadata":{}},{"cell_type":"code","source":"# Importing libraries \nimport numpy as np\nimport random\nimport math\nimport random \nfrom collections import deque\nimport collections\nimport pickle\n\n# for building DQN \nimport tensorflow as tf\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\n# for plotting graphs\nimport matplotlib.pyplot as plt\n\n# Clearing outputs\nfrom IPython.display import clear_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the time matrix\n#### Also, change some plotting parameters\n---------","metadata":{}},{"cell_type":"code","source":"# Loading the time matrix provided\nTime_matrix = np.load(\"../input/rl-project-files/TM_lower.npy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the plotting size\nplt.rcParams[\"figure.figsize\"] = (10,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Environment","metadata":{}},{"cell_type":"code","source":"# Defining hyperparameters\nm = 5 # number of cities, ranges from 1 ..... m\nt = 24 # number of hours, ranges from 0 .... t-1\nd = 7  # number of days, ranges from 0 ... d-1\nC = 5 # Per hour fuel and other costs\nR = 9 # per hour revenue from a passenger\n\nclass CabDriver():\n    \n    '''Constructor function for the environment...'''\n    def __init__(self):\n        # Create the action space\n        self.action_space = [[0,0]] + [[i,j] for i in range(1, m+1) for j in range(1, m+1) if i != j]\n        \n        # Create the state space \n        self.state_space = [[i,j,k] for i in range (1,m+1) for j in range(t) for k in range(d)]\n        \n        # Choose a random state\n        self.state_init = self.state_space[random.randrange(0, len(self.state_space))]\n\n        # Start the first round\n        self.reset()\n\n\n    ''' Function for converting the state into a one-hot encoded vector.\n        \n        @params: state: State to be one-hot encoded. It is a list [location, time, day]\n        @returns: One-hot encoded form of given state\n        \n    '''\n    def state_encod_arch2(self, state):\n        location = state[0] # Store the current location\n        time = state[1]     # Store the current time\n        day = state[2]      # Store the current day\n        \n        # Create a vector for encoding the state\n        state_encod = np.zeros((m+t+d, 1))\n        \n        # Encode the location information. Notice that location is from [1,2,3,4,5], so we need\n        # to subtract 1 from the current location to one-hot encode it properly. \n        state_encod[location-1][0] = 1 \n        \n        # Encode the time information\n        state_encod[m+time][0] = 1\n        \n        # Encode the day information\n        state_encod[m+t+day][0] = 1\n        \n        # Return the state encoding\n        return state_encod\n\n    ''' Function for getting requests given a state. The number of requests\n        is decided by the MDP formulation given in the problem statement. \n        \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @returns: possible_actions_index: List of indices of possible actions.\n                  These indices correspond to the index of that action in the action space\n        @returns: actions: The actual actions (a list of lists, with each inner list consisting of two locations)\n        \n    '''\n    def requests(self, state):\n        # Determining the number of requests basis the location. \n        # Get the current location\n        location = state[0]\n        \n        # Set number of requests to 0 for now...\n        requests = 0\n        \n        # Get the number of requests as per MDP specified in problem statement\n        if location == 1:\n            requests = np.random.poisson(2)\n        elif location == 2:\n            requests = np.random.poisson(12)\n        elif location == 3:\n            requests = np.random.poisson(4)\n        elif location == 4:\n            requests = np.random.poisson(7)\n        elif location == 5:\n            requests = np.random.poisson(8)\n\n        # Cap the total number of requests at 15\n        if requests > 15:\n            requests = 15\n\n        # Randomly choose requests from the action space. Note that (0,0) is not a customer request. \n        possible_actions_index = random.sample(range(1, (m-1)*m +1), requests) \n        \n        # Get the actions corresponding to those indices\n        actions = [self.action_space[i] for i in possible_actions_index]\n        \n        # Finally append the lazy action - where the driver doesn't take any request at all\n        possible_actions_index.append(0)\n        actions.append([0,0])\n\n        # Return the list of action indices, and the actions corresponding to those indices. \n        return possible_actions_index, actions   \n\n    ''' This function gives out the reward for an action taken at a particular state\n    \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @params: action: The action taken from the action space \n        @params: Time_matrix: The time matrix that is used for deciding the time taken to go \n                 from one location to another, at a given time and a given day\n                 \n        @returns: A scalar, which is the reward the agent gets for taking the corresponding \n                  action from the given state. \n                  \n    '''\n    def reward_func(self, state, action, Time_matrix):\n        # If the driver decides to not take any ride, he still incurs the fuel cost...\n        if(action == [0,0]):\n            return -1*C\n        # However, if the driver decides to take a ride, we should calculate the reward he gets...\n        else:\n            # time_ip is the amount of time taken by driver to go from his current location to pickup location\n            time_ip = int(Time_matrix[state[0]-1][action[0]-1][state[1]][state[2]])\n            \n            # Once the driver reaches the pickup location, we will have a new time and day. So we calculate that...\n            new_time = state[1] + time_ip \n            new_day = state[2]\n            new_day = (new_day + (new_time // 24)) % 7\n            new_time = new_time % 24 \n            \n            # time_pq is the amount of tiem taken by the driver to go from his pickup location to destination location\n            time_pq = int(Time_matrix[action[0]-1][action[1]-1][new_time][new_day])\n            \n            # Compute the reward as specified in the MDP\n            reward = R * time_pq - C * (time_pq + time_ip)\n            \n            # Return the reward... \n            return reward\n    \n    ''' Function for determining the next state, given the current state.\n        \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @params: action: The action taken from the action space \n        @params: Time_matrix: The time matrix that is used for deciding the time taken to go \n                 from one location to another, at a given time and a given day\n        @returns: The next state and the time elapsed for taking this action.\n        \n    '''\n    def next_state_func(self, state, action, Time_matrix):\n        # If the driver decided to not entertain any ride, his location is same\n        # Only the time (and possibly the day) gets increased by one hour\n        if(action == [0,0]):\n            # New location is same as old location\n            new_location = state[0]   \n            \n            # Compute the new time \n            new_time = state[1] + 1\n            \n            # Compute the new day\n            new_day = state[2]         \n            new_day = new_day + (new_time//24)\n            \n            # Take modulos of new_day and new_time to ensure they are within limits... \n            new_day = new_day % 7\n            new_time = new_time % 24\n            \n            # Create the new state\n            new_state = [new_location ,new_time, new_day]\n            \n            # Return the new state and action\n            return [new_state, 1] # 1 hour is the time elapsed\n        else:\n            # As usual, time_ip is the time taken by driver to go from current location to pickup location\n            time_ip = int(Time_matrix[state[0]-1][action[0]-1][state[1]][state[2]])\n            \n            # Compute the new time and day, since we will use updated state to compute the time taken for \n            # going from pickup location to destination location... \n            new_time = state[1] + time_ip\n            new_day = state[2]\n            new_day = (new_day + (new_time // 24)) % 7\n            new_time = new_time % 24 \n            \n            # Again, time_pq is the time taken to go from pickup location to destination location\n            time_pq = int(Time_matrix[action[0]-1][action[1]-1][new_time][new_day])\n            \n            # Compute the total time elapsed...\n            total_time = time_ip + time_pq \n            \n            # Compute the new state\n            new_location = action[1]\n            new_time = state[1] + total_time\n            new_day = state[2]\n            new_day = ((new_day + (new_time//24))%7)\n            new_time = new_time % 24 \n            next_state = [new_location, new_time, new_day]\n            \n            # Return the new state and total time elapsed...\n            return [next_state, total_time] \n\n    ''' Function for resetting the environment. \n        \n        @returns: The action space, state space and the initial space (not one-hot encoded) \n        \n    '''\n    def reset(self):\n        return self.action_space, self.state_space, self.state_init","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the agent\n----------------------","metadata":{}},{"cell_type":"code","source":"class DQNAgent:\n    ''' Constructor for the DQN Agent. \n        \n        @params: state_size: The size of the one-hot encoded state-vector\n        @params: action_size: The size of the action space\n    '''\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size     # Specify the state size\n        self.action_size = action_size   # Specify the action size\n\n        self.discount_factor = 0.95      # The discount factor (gamma)\n        self.learning_rate = 0.01        # The learning rate for training the NN\n        self.epsilon = 0.999             # Initial epsilon, high epsilon means high exploration  \n        self.epsilon_decay = 0.995       # The epsilon decay factor. epsilon is multiplied by epsilon_decay every 4 episodes \n        self.epsilon_min = 0.01          # Minimum epsilon. The epsilon cannot go below this   \n        \n        self.batch_size = 128            # The batch size for training\n        \n        self.memory = deque(maxlen=2000) # This structure is used as the memory replay, storing [s,a,r,s',done]\n\n        # Create the model to be used by the agent. It would be a neural network\n        self.model = self.build_model()\n\n    ''' This function builds the Neural Network using which will be deciding the \n        the actions of the agent. \n        \n        @returns: model: The Keras Neural Network model\n    '''\n    def build_model(self):\n        # We create a sequential model \n        model = Sequential()\n        \n        # Create the layers. There are 3 layers, with the first two layers having\n        # ReLU activation function and the last one having a linear activation function\n        model.add(Dense(64, input_dim = self.state_size, activation = tf.keras.layers.ReLU()))\n        model.add(Dense(32, activation = tf.keras.layers.ReLU()))\n        model.add(Dense(self.action_size, activation = 'linear'))\n        \n        # Compile the model. We will be using Adam as the optimizer, with MSE as the loss\n        model.compile(loss='mse', optimizer=Adam(learning_rate = self.learning_rate))\n        \n        # Return the model\n        return model\n\n    ''' Function for converting the state into a one-hot encoded vector.\n \n         This function is identical to the state encoder of the environment, except the fact\n         that the one-hot encoded vector of the state being returned is a horizontal vector\n         instead of a vertical vector. \n     \n         @params: state: State to be one-hot encoded. It is a list [location, time, day]\n         @returns: One-hot encoded form of given state\n \n    '''\n    def state_encoder(self, state):\n        location = state[0]       # Store the current location\n        time = state[1]           # Store the current time\n        day = state[2]            # Store the current day\n        \n        # Create the one hot encoded vector\n        state_encod = np.zeros((m+t+d, 1)) \n        \n        state_encod[location-1][0] = 1   # One-hot encode the location     \n        state_encod[m+time][0] = 1       # one-hot encode the time\n        state_encod[m+t+day][0] = 1      # One-hot encode the day\n        \n        # Make the vector a horizontal vector\n        state_encod = state_encod.reshape((1, state_encod.shape[0]))\n        \n        # Return the one-hot encoded vector\n        return state_encod\n\n    ''' This function is used for getting the action from the model. This is, in some\n        sense, the policy. To be more precise, it is an epsilon-greedy policy. The \n        epsilon can be controlled using the parameters. \n    \n        @params: state: State in which te driver is currently at.\n        @params: actions_ind: The list of possible action indices.\n        @returns: the action index that is being taken by the policy... \n    '''\n    def get_action(self, state, actions_ind):\n        # Check if the action we need to take is a randomized one... \n        if np.random.rand() <= self.epsilon:\n            return random.choice(actions_ind)\n        \n        # If not, we take the best action as per the model\n        else:\n            # Encode the state\n            encoded_state = self.state_encoder(state)    \n            \n            # Get the Q value of all actions for the states\n            q_value = self.model.predict(encoded_state)  \n            \n            # Filter out the Q-values to include only action which can be taken\n            required_q_values = [q_value[0][j] for j in actions_ind]   \n\n            # Return the index of the maximum action (having maximum Q-value) that the model predicts \n            return actions_ind[np.argmax(required_q_values)]  \n\n    ''' This function appends a sample into the memory replay buffer.\n    \n        Note that we don't decay the epsilon every time a sample has been appended, we will\n        rather decay them once the episode ends. Otherwise, our epsilon would decay very \n        fast. \n        \n        @params: state: The state (not one-hot encoded) to be pushed to the buffer\n        @params: action: The index of the action (in sync with that in action_space) being taken\n        @params: reward: The reward obtained for taking the given action from the given state\n        @params: next_state: The next state to which the agent goes after taking action from state \n        @params: done: A boolean. True means the episode has ended, otherwise False. \n        \n    '''\n    def append_sample(self, state, action, reward, next_state, done):\n        # Append <s,a,r,s',done> to the mmeory replay buffer\n        self.memory.append((state, action, reward, next_state, done))\n    \n    ''' This function trains the model using samples from the memory replay buffer\n        It doesn't take in any parameters and doesn't return anything\n        \n    '''\n    def train_model(self):\n        # Perform training only if enough samples are available\n        if len(self.memory) > self.batch_size:\n            \n            # Sample batch from the memory\n            mini_batch = random.sample(self.memory, self.batch_size)\n            \n            # Create arrays holding the states s and s' in a sample <s,a,r,s',done>\n            update_output = np.zeros((self.batch_size, self.state_size))\n            update_input = np.zeros((self.batch_size, self.state_size))\n            \n            # Arrays for storing the actions, rewards, and dones for different samples\n            action, reward, done = [], [], []\n            \n            # Now, we start processing the samples from the batch\n            for i in range(self.batch_size):\n                # Get the <s,a,r,s',done> for the i'th entry from the mini batch\n                c_state, c_action, c_reward, c_next_state, c_done = mini_batch[i]\n                \n                # Append the action, reward, done for the current sample\n                action.append(c_action)\n                reward.append(c_reward)\n                done.append(c_done)\n                \n                # Create state encoding for initial state\n                update_input[i] = self.state_encoder(c_state)\n                \n                # Create encoding for the final state\n                update_output[i] = self.state_encoder(c_next_state)\n                \n            # Predict the Q-values for given states (s) of the mini-batch\n            target = self.model.predict(update_input)\n            \n            # Predict the Q-values for the given next states (s') of the mini-batch\n            Q_output_states = self.model.predict(update_output)\n            \n            # Now, update the targets for the actions taken from state s\n            for i in range(self.batch_size):\n                if(done[i]):\n                    target[i][action[i]] = reward[i]\n                else:\n                    target[i][action[i]] = reward[i] + self.discount_factor * np.max(Q_output_states[i])              \n                \n            # Fit the model and track the loss values\n            self.model.fit(update_input, target, batch_size = self.batch_size, epochs = 1, verbose = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DQN block","metadata":{}},{"cell_type":"code","source":"# Define total number of episodes  \ntotal_episodes = 1900\n\n# Create a temporary environment. This environment is used for getting the action space and state space\nenv_tmp = CabDriver()\n\n# Getting the action and state space from the temporary environment\naction_space, state_space, initial_state = env_tmp.reset()\n\n# Define state size\nstate_sz = m + t + d\n\n# Define the size of action space\naction_sz = len(action_space)\n\n# Call the DQN Agent\nagent = DQNAgent(state_sz, action_sz)\n\n# Create a list for tracking rewards. We use it for plotting purposes. \nreward_tracker = []\n\n# Print the agent model summary\nprint(agent.model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mention the state whose Q-values would be tracked\nstate_to_track = [1,0,0]\ntrack_state = agent.state_encoder(state_to_track)\n\n# Mention the action index whose Q-values are to be tracked. So, we track Q(state_to_track, action_space[track_action_dx])\ntrack_action_idx = 1 # This is the action [1, 2]\n\n# Create a list for tracking Q-values. We use it for plotting purposes. \nqval_tracker = [] \n\n''' Function for getting the Q-value for given state, action pair.\n    \n    @params: track_state: State (not one-hot encoded) whose Q-value is needed\n    @params: track_action_idx: The action index (as in action_space) being taken. \n    @returns: Returns Q-value Q(track_state, action_space[track_action_dx])\n'''\ndef get_qval(track_state, track_action_idx):\n    # Predict the Q-values of all actions possible from track_state\n    res_qval = agent.model.predict(track_state)\n    \n    # Get the Q-value for the action required\n    qval = res_qval[0][track_action_idx]\n    \n    # Return the corresponding Q-value\n    return qval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for episode in range(total_episodes):\n    # Print the current episode\n    print(\"In episode \" + str(episode))\n    \n    # Call the environment\n    env = CabDriver()\n    \n    # Call all the initialised variables of the environment\n    action_space, state_space, current_state = env.reset()\n    \n    # Variable for keeping track of whether the episode has ended or not\n    done = False\n    \n    total_time = 0  # Total time elapsed for the current episode\n    sum_rew = 0   # Total reward obtained in the current episode \n    \n    # Now, run the episode\n    while done == False:\n        # Get the actions possible from the current state\n        actions_possible_ind, possible_action = env.requests(current_state)\n        \n        # Get the action index that the agent would like to take\n        agent_action_index = agent.get_action(current_state, actions_possible_ind)\n        \n        # Get the actual action from the agent_action_index and the action_space\n        agent_action = action_space[agent_action_index]\n        \n        # Get the reward for taking this action from the current state\n        reward = env.reward_func(current_state, agent_action, Time_matrix)\n        \n        # Update total reward obtained\n        sum_rew += reward\n        \n        # Get the new state and the total time elapsed\n        new_state, time_elapsed = env.next_state_func(current_state, agent_action, Time_matrix)\n        \n        # Update the total time elapsed\n        total_time += time_elapsed\n        \n        # If the agent took action that exceeded maximum time, ignore that action\n        if(total_time >= 24*30):\n            done = True\n            \n        # Otherwise, append the sample, train the model and set the current state as the new state\n        else:\n            agent.append_sample(current_state, agent_action_index, reward, new_state, done)\n            agent.train_model()\n            current_state = new_state\n    \n    # Every 4 episodes, we do the decay of epsilon.\n    # By doing so, at around 1900 episodes, the epsilon would 0.1\n    if(((episode + 1) % 4) == 0):\n        if(agent.epsilon > agent.epsilon_min):\n            agent.epsilon *= agent.epsilon_decay\n         \n    # To keep notebook clean, we just clear out any plots that we \n    # have plotted after every 100 episodes \n    if(((episode + 1) % 100)  == 0):\n        clear_output(wait = True)\n        \n    # Append the reward to reward_tracker\n    reward_tracker.append(sum_rew)\n    \n    # Append the Q-value to the Q-value tracker.\n    qval_tracker.append(get_qval(track_state, track_action_idx))\n    \n    # Plot the rewards and Q-values after every 20 episodes.\n    if(((episode+1) % 20) == 0):\n        # Plot the rewards\n        plt.plot(reward_tracker)\n        title = \"Current Epsilon: \" + str(agent.epsilon)\n        plt.title(title)\n        plt.show()\n        \n        # Plot the Q-values\n        plt.plot(qval_tracker)\n        title = \"QValue Plot for state \" + str(state_to_track) + \" for action \" + str(action_space[track_action_idx])\n        plt.title(title)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the converged model as a h5 file. \n# I didn't use pickle because it failed (pickle works for sequences only...)\nagent.model.save('Converged_1900.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T06:22:43.147000Z","iopub.execute_input":"2022-04-28T06:22:43.147288Z","iopub.status.idle":"2022-04-28T06:22:43.185837Z","shell.execute_reply.started":"2022-04-28T06:22:43.147258Z","shell.execute_reply":"2022-04-28T06:22:43.184648Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Function for plotting smoothened form of the reward function\n# This code was taken from: https://stackoverflow.com/questions/5283649/plot-smooth-line-with-pyplot\n\ndef smooth(scalars, weight):  # Weight between 0 and 1\n    last = scalars[0]  # First value in the plot (first timestep)\n    smoothed = list()\n    for point in scalars:\n        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n        smoothed.append(smoothed_val)                        # Save it\n        last = smoothed_val                                  # Anchor the last smoothed value\n        \n    return smoothed\n\n\nplt.plot(smooth(reward_tracker, 0.95))\nplt.title(\"Smooth plot of rewards obtained\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot of epsilon-decay function being used\n------------","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\nTry building a similar epsilon-decay function for your model.\n</div>","metadata":{}},{"cell_type":"code","source":"# List to keep track of values\ntrack_values = [1]\n\n# Loop for simulating the epsilon-decay\nfor i in range(total_episodes):\n    last_val = track_values[-1]\n    if((i+1) % 4 == 0):\n        new_val = last_val * agent.epsilon_decay\n        track_values.append(new_val)\n    else:\n        track_values.append(last_val)\n        \n# Plot the decay portion. Note that the decay is applied\n# after every 4 episodes...\nplt.plot(track_values)\nplt.title(\"Epsilon decay plot\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------","metadata":{}}]}