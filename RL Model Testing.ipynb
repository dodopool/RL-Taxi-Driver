{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import important libraries\n---------","metadata":{}},{"cell_type":"code","source":"# Importing libraries \nimport numpy as np\nimport random\nimport math\nimport random \nfrom collections import deque\nimport collections\nimport pickle\n\n# for building DQN \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.097964Z","iopub.execute_input":"2022-04-28T01:53:12.098312Z","iopub.status.idle":"2022-04-28T01:53:12.10535Z","shell.execute_reply.started":"2022-04-28T01:53:12.09828Z","shell.execute_reply":"2022-04-28T01:53:12.104376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Time Matrix \n#### Also, we load the model\n--------------","metadata":{}},{"cell_type":"code","source":"# Loading the time matrix provided\nTime_matrix = np.load(\"../input/rl-project-files/TM_lower.npy\")\n\n# Loading the model\nmodel = keras.models.load_model('../input/trained-model/Converged_1900.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.181223Z","iopub.execute_input":"2022-04-28T01:53:12.18185Z","iopub.status.idle":"2022-04-28T01:53:12.268389Z","shell.execute_reply.started":"2022-04-28T01:53:12.181811Z","shell.execute_reply":"2022-04-28T01:53:12.267625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Environment\n#### The environment is the MDP described in problem statement\n----------","metadata":{}},{"cell_type":"code","source":"# Defining hyperparameters\nm = 5 # number of cities, ranges from 1 ..... m\nt = 24 # number of hours, ranges from 0 .... t-1\nd = 7  # number of days, ranges from 0 ... d-1\nC = 5 # Per hour fuel and other costs\nR = 9 # per hour revenue from a passenger\n\nclass CabDriver():\n    \n    '''Constructor function for the environment...'''\n    def __init__(self):\n        # Create the action space\n        self.action_space = [[0,0]] + [[i,j] for i in range(1, m+1) for j in range(1, m+1) if i != j]\n        \n        # Create the state space \n        self.state_space = [[i,j,k] for i in range (1,m+1) for j in range(t) for k in range(d)]\n        \n        # Choose a random state\n        self.state_init = self.state_space[random.randrange(0, len(self.state_space))]\n\n        # Start the first round\n        self.reset()\n\n\n    ''' Function for converting the state into a one-hot encoded vector.\n        \n        @params: state: State to be one-hot encoded. It is a list [location, time, day]\n        @returns: One-hot encoded form of given state\n        \n    '''\n    def state_encod_arch1(self, state):\n        location = state[0] # Store the current location\n        time = state[1]     # Store the current time\n        day = state[2]      # Store the current day\n        \n        # Create a vector for encoding the state\n        state_encod = np.zeros((m+t+d, 1))\n        \n        # Encode the location information. Notice that location is from [1,2,3,4,5], so we need\n        # to subtract 1 from the current location to one-hot encode it properly. \n        state_encod[location-1][0] = 1 \n        \n        # Encode the time information\n        state_encod[m+time][0] = 1\n        \n        # Encode the day information\n        state_encod[m+t+day][0] = 1\n        \n        # Return the state encoding\n        return state_encod\n\n    ''' Function for getting requests given a state. The number of requests\n        is decided by the MDP formulation given in the problem statement. \n        \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @returns: possible_actions_index: List of indices of possible actions.\n                  These indices correspond to the index of that action in the action space\n        @returns: actions: The actual actions (a list of lists, with each inner list consisting of two locations)\n        \n    '''\n    def requests(self, state):\n        # Determining the number of requests basis the location. \n        # Get the current location\n        location = state[0]\n        \n        # Set number of requests to 0 for now...\n        requests = 0\n        \n        # Get the number of requests as per MDP specified in problem statement\n        if location == 1:\n            requests = np.random.poisson(2)\n        elif location == 2:\n            requests = np.random.poisson(12)\n        elif location == 3:\n            requests = np.random.poisson(4)\n        elif location == 4:\n            requests = np.random.poisson(7)\n        elif location == 5:\n            requests = np.random.poisson(8)\n\n        # Cap the total number of requests at 15\n        if requests > 15:\n            requests = 15\n\n        # Randomly choose requests from the action space. Note that (0,0) is not a customer request. \n        possible_actions_index = random.sample(range(1, (m-1)*m +1), requests) \n        \n        # Get the actions corresponding to those indices\n        actions = [self.action_space[i] for i in possible_actions_index]\n        \n        # Finally append the lazy action - where the driver doesn't take any request at all\n        possible_actions_index.append(0)\n        actions.append([0,0])\n\n        # Return the list of action indices, and the actions corresponding to those indices. \n        return possible_actions_index, actions   \n\n    ''' This function gives out the reward for an action taken at a particular state\n    \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @params: action: The action taken from the action space \n        @params: Time_matrix: The time matrix that is used for deciding the time taken to go \n                 from one location to another, at a given time and a given day\n                 \n        @returns: A scalar, which is the reward the agent gets for taking the corresponding \n                  action from the given state. \n                  \n    '''\n    def reward_func(self, state, action, Time_matrix):\n        # If the driver decides to not take any ride, he still incurs the fuel cost...\n        if(action == [0,0]):\n            return -1*C\n        # However, if the driver decides to take a ride, we should calculate the reward he gets...\n        else:\n            # time_ip is the amount of time taken by driver to go from his current location to pickup location\n            time_ip = int(Time_matrix[state[0]-1][action[0]-1][state[1]][state[2]])\n            \n            # Once the driver reaches the pickup location, we will have a new time and day. So we calculate that...\n            new_time = state[1] + time_ip \n            new_day = state[2]\n            new_day = (new_day + (new_time // 24)) % 7\n            new_time = new_time % 24 \n            \n            # time_pq is the amount of tiem taken by the driver to go from his pickup location to destination location\n            time_pq = int(Time_matrix[action[0]-1][action[1]-1][new_time][new_day])\n            \n            # Compute the reward as specified in the MDP\n            reward = R * time_pq - C * (time_pq + time_ip)\n            \n            # Return the reward... \n            return reward\n    \n    ''' Function for determining the next state, given the current state.\n        \n        @params: state: State (not in one-hot encoded form). It is a list [location, time, day]\n        @params: action: The action taken from the action space \n        @params: Time_matrix: The time matrix that is used for deciding the time taken to go \n                 from one location to another, at a given time and a given day\n        @returns: The next state and the time elapsed for taking this action.\n        \n    '''\n    def next_state_func(self, state, action, Time_matrix):\n        # If the driver decided to not entertain any ride, his location is same\n        # Only the time (and possibly the day) gets increased by one hour\n        if(action == [0,0]):\n            # New location is same as old location\n            new_location = state[0]   \n            \n            # Compute the new time \n            new_time = state[1] + 1\n            \n            # Compute the new day\n            new_day = state[2]         \n            new_day = new_day + (new_time//24)\n            \n            # Take modulos of new_day and new_time to ensure they are within limits... \n            new_day = new_day % 7\n            new_time = new_time % 24\n            \n            # Create the new state\n            new_state = [new_location ,new_time, new_day]\n            \n            # Return the new state and action\n            return [new_state, 1] # 1 hour is the time elapsed\n        else:\n            # As usual, time_ip is the time taken by driver to go from current location to pickup location\n            time_ip = int(Time_matrix[state[0]-1][action[0]-1][state[1]][state[2]])\n            \n            # Compute the new time and day, since we will use updated state to compute the time taken for \n            # going from pickup location to destination location... \n            new_time = state[1] + time_ip\n            new_day = state[2]\n            new_day = (new_day + (new_time // 24)) % 7\n            new_time = new_time % 24 \n            \n            # Again, time_pq is the time taken to go from pickup location to destination location\n            time_pq = int(Time_matrix[action[0]-1][action[1]-1][new_time][new_day])\n            \n            # Compute the total time elapsed...\n            total_time = time_ip + time_pq \n            \n            # Compute the new state\n            new_location = action[1]\n            new_time = state[1] + total_time\n            new_day = state[2]\n            new_day = ((new_day + (new_time//24))%7)\n            new_time = new_time % 24 \n            next_state = [new_location, new_time, new_day]\n            \n            # Return the new state and total time elapsed...\n            return [next_state, total_time] \n\n    ''' Function for resetting the environment. \n        \n        @returns: The action space, state space and the initial space (not one-hot encoded) \n        \n    '''\n    def reset(self):\n        return self.action_space, self.state_space, self.state_init","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.273813Z","iopub.execute_input":"2022-04-28T01:53:12.274297Z","iopub.status.idle":"2022-04-28T01:53:12.309155Z","shell.execute_reply.started":"2022-04-28T01:53:12.274224Z","shell.execute_reply":"2022-04-28T01:53:12.308261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for getting actions\n#### These functions allow us to get action from the trained model...\n------","metadata":{}},{"cell_type":"code","source":"''' Function for converting the state into a one-hot encoded vector.\n \n     This function is identical to the state encoder of the environment, except the fact\n     that the one-hot encoded vector of the state being returned is a horizontal vector\n     instead of a vertical vector. \n     \n     @params: state: State to be one-hot encoded. It is a list [location, time, day]\n     @returns: One-hot encoded form of given state\n     \n'''\ndef state_encoder(state):\n    location = state[0] # Store the current location\n    time = state[1]     # Store the current time\n    day = state[2]      # Store the current day\n        \n    # Create a one-hot encoded vector\n    state_encod = np.zeros((m+t+d, 1))\n    state_encod[location-1][0] = 1\n    state_encod[m+time][0] = 1\n    state_encod[m+t+day][0] = 1\n    \n    # Make the vector a horizontal vector\n    state_encod = state_encod.reshape((1, state_encod.shape[0]))\n    \n    # Return the state encoding\n    return state_encod\n\n''' This function is used for getting the action from the model. This is, in some\n    sense, the policy. To be more precise, it is an epsilon-greedy policy. The \n    epsilon can be controlled using the parameters. \n    \n    @params: state: State in which the driver is currently at.\n    @params: actions_ind: The list of possible action indices.\n    @params: epsilon: The epsilon required for the epsilon-greedy policy. Higher policy means more randomness. \n    @returns: the action index that is being taken by the policy... \n    \n'''\ndef get_action(state, actions_ind, epsilon = 0.1):\n    # Decide if a random action should be chosen\n    if np.random.rand() <= epsilon:\n        return random.choice(actions_ind) # Return the random action\n    # Otherwise, choose the action as per the learned model\n    else:\n        # Encode the current state... \n        encoded_state = state_encoder(state)\n            \n        # Get the Q values of all the actions\n        q_value = model.predict(encoded_state)\n        \n        # Keep only those Q-values which corresponds to actions that can be taken\n        required_q_values = [q_value[0][j] for j in actions_ind]\n\n        # Return the corresponding action index which will be taken\n        return actions_ind[np.argmax(required_q_values)]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.39379Z","iopub.execute_input":"2022-04-28T01:53:12.394819Z","iopub.status.idle":"2022-04-28T01:53:12.406825Z","shell.execute_reply.started":"2022-04-28T01:53:12.394776Z","shell.execute_reply":"2022-04-28T01:53:12.404986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code for checking model performance\n#### We will see the rewards the model gets\n--------","metadata":{}},{"cell_type":"code","source":"# Create a temporary environment for getting actio space and state space\nenv_tmp = CabDriver()\n\n# Getting the action and state space, as well as the initial state\naction_space, state_space, initial_state = env_tmp.reset()\n\n# Define state size\nstate_sz = m + t + d\naction_sz = len(action_space)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.53515Z","iopub.execute_input":"2022-04-28T01:53:12.535531Z","iopub.status.idle":"2022-04-28T01:53:12.543908Z","shell.execute_reply.started":"2022-04-28T01:53:12.535488Z","shell.execute_reply":"2022-04-28T01:53:12.542356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an environment\nenv = CabDriver()\n    \n# Reset the environment\naction_space, state_space, current_state = env.reset()\n    \n# Below variable keeps track of whether the episode has ended or not... \ndone = False\n\n# Keeps track of total time elapsed\ntotal_time = 0\n\n# Keeps track of total reward obtained\nsum_rew = 0\n\n# Now start the episode\nwhile done == False:\n    # Get the actions from the environment\n    actions_possible_ind, possible_action = env.requests(current_state)\n       \n    # Get the action as per the epsilon-greedy policy\n    agent_action_index = get_action(current_state, actions_possible_ind)\n     \n    # Get the actual agent action\n    agent_action = action_space[agent_action_index]\n    \n    # Get the reward and add to tal reward\n    reward = env.reward_func(current_state, agent_action, Time_matrix)\n    sum_rew += reward\n    \n    # Get the new state and the total time elapsed\n    new_state, time_elapsed = env.next_state_func(current_state, agent_action, Time_matrix)\n    \n    # Update total time elapsed\n    total_time += time_elapsed\n    \n    # If episode has ended, set done to True\n    if(total_time >= 24*30):\n        done = True\n    # Else, set the current state as the new state\n    else:\n        current_state = new_state","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:12.63938Z","iopub.execute_input":"2022-04-28T01:53:12.639682Z","iopub.status.idle":"2022-04-28T01:53:19.800682Z","shell.execute_reply.started":"2022-04-28T01:53:12.639652Z","shell.execute_reply":"2022-04-28T01:53:19.799525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the reward the agent got\nprint(\"Total reward agent got: \", sum_rew)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T01:53:19.803496Z","iopub.execute_input":"2022-04-28T01:53:19.80383Z","iopub.status.idle":"2022-04-28T01:53:19.809021Z","shell.execute_reply.started":"2022-04-28T01:53:19.803797Z","shell.execute_reply":"2022-04-28T01:53:19.808293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------","metadata":{}}]}